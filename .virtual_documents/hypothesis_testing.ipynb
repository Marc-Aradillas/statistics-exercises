






































import numpy as np
import pandas as pd

from pydataset import data
from scipy import stats
from scipy.stats import chi2_contingency











# made the table to use the data
data = {
    'Codeup Student': [49, 1],
    'Not Codeup Student': [20, 30]
}
contingency_table = pd.DataFrame(data, index=['Uses a Macbook', "Doesn't Use A Macbook"])


#implemened the chi squared contingency test
chi, p, degf, exp = stats.chi2_contingency(contingency_table)


chi


p


degf


exp


contingency_table


a = 0.05


if p < a:
    print('We reject the null hypothesis. There appears to be a relationship.')
else: print('We failed to reject the null hypothesis.')








import numpy as np
import pandas as pd

from pydataset import data
from scipy import stats
from scipy.stats import chi2_contingency

#imported data
mpg = data('mpg')
mpg.head()


mpg.nunique()


mpg.cyl.value_counts()


mpg.drv.value_counts()











#stated alpha
a = 0.05





observed = pd.crosstab(mpg.drv, mpg.cyl)
observed


#implemened the chi squared contingency test
chi, p, degf, exp = stats.chi2_contingency(observed)








#output values
print('Observed')
print(observed.values)
print('\nExpected')
print(exp.astype(int))
print('\n----')
print(f'chi^2 = {chi:.4f}')
print(f'p     = {p}')





chi


p


degf


exp





if p < a:
    print('We reject the null hypothesis. There appears to be a relationship.')
else: print('We failed to reject the null hypothesis.')





# DS Libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy import stats
from pydataset import data
from env import get_connection
from sqlalchemy import create_engine, text


def get_db_url(database):
    return get_connection(database)


url = get_db_url('employees')
query = text('SELECT * FROM employees')
engine = create_engine(url)
employees = pd.read_sql(query, engine.connect())


employees.head()





url = get_db_url('employees')
query = text('''
        SELECT *
        FROM employees AS e 
        JOIN dept_emp AS de ON e.emp_no = de.emp_no 
        JOIN departments AS d ON de.dept_no = d.dept_no
        WHERE to_date = '9999-01-01'
        ''')
engine = create_engine(url)
employees = pd.read_sql(query, engine.connect())


employees.head()


employees = employees.drop(['emp_no', 'dept_no', 'birth_date', 'hire_date', 'from_date', 'to_date', 'first_name', 'last_name'], axis=1)


employees['emp_dept'] = np.where(employees.dept_name.str.startswith('S'), 'Sales', 'Marketing')
employees.head()


employees = employees.drop(['dept_name'], axis=1)


employees.head()


employees.gender.value_counts()


gender_dept_correlation = pd.crosstab(employees.gender, employees.emp_dept)
gender_dept_correlation 


#implemened the chi squared contingency test
chi, p, degf, exp = stats.chi2_contingency(gender_dept_correlation)


chi


p


degf


exp


#set alpha
a = 0.05


if p < a:
    print('We reject the null hypothesis. There appears to be a relationship.')
else: print('We failed to reject the null hypothesis.')


























# DS Libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from pydataset import data
from env import get_connection
from sqlalchemy import create_engine, text








# definining eval result function
def eval_result(p_value, a = 0.05):
    
    if p_value < a:

        print('Awesome Sauce! Your result is significant!')
        
    else: 

        print('Your result was not significant!')


def get_db_url(database):
    return get_connection(database)


url = get_db_url('telco_churn')
query = text('SELECT * FROM customers, internet_service_types')
engine = create_engine(url)
tc = pd.read_sql(query, engine.connect())


tc.nunique()


tc.info()


tc.head()


#always create a visual to accomodate the stats tests

plt.scatter(tc.monthly_charges, tc.tenure, color='chocolate', alpha=0.5, s=7)
plt.xlabel('Monthly Charges')
plt.ylabel('tenure')
plt.title('Customer retention based on monthly charges')

plt.grid()
plt.show()


r, p = stats.pearsonr(tc.monthly_charges, tc.tenure)
r, p

# we can see the correlation will not be very linear based on value returned


eval_result(p)











# always create a visual to accomodate the stats tests
# tc['total_charges'] = pd.to_numeric(tc['total_charges'])

tc_cleaned = tc.dropna(subset=['total_charges'])
tc_cleaned['total_charges'] = pd.to_numeric(tc_cleaned['total_charges'], errors='coerce')

# Drop rows with NaNs in 'total_charges' or 'tenure'
tc_cleaned = tc_cleaned.dropna(subset=['total_charges'])


plt.scatter(tc_cleaned.tenure, tc_cleaned.total_charges, color='fuchsia', alpha=0.3, s=10)
plt.xlabel('Total Charges')
plt.ylabel('tenure')
plt.title('Customer tenure based on total charges')

plt.grid()
plt.show()


# Checked for infinities and NaNs in 'total_charges' and 'tenure' columns
print("Infinities in 'total_charges':", np.isinf(tc_cleaned['total_charges']).any())
print("NaNs in 'total_charges':", np.isnan(tc_cleaned['total_charges']).any())

print("Infinities in 'tenure':", np.isinf(tc_cleaned['tenure']).any())
print("NaNs in 'tenure':", np.isnan(tc_cleaned['tenure']).any())









# tc['total_charges'] = tc['total_charges'].replace('', np.nan)

tc['total_charges'] = tc['total_charges'].replace(r'^\s*$', np.nan, regex=True)
tc['total_charges'] = tc['total_charges'].astype('float')


plt.scatter(tc.tenure, tc.total_charges, color='fuchsia', alpha=0.3, s=10)
plt.xlabel('Total Charges')
plt.ylabel('tenure')
plt.title('Customer tenure based on total charges')

plt.grid()
plt.show()





r, p = stats.pearsonr(tc_cleaned.total_charges, tc_cleaned.tenure)
r, p


eval_result(p)





tc.head()


# Boolean mask to control
tc = tc[(tc['phone_service'] == 'Yes') & (tc['internet_service_type'] == 'DSL')]


# tc['total_charges'] = tc['total_charges'].replace('', np.nan)

tc['total_charges'] = tc['total_charges'].replace(r'^\s*$', np.nan, regex=True)
tc['total_charges'] = tc['total_charges'].astype('float')


#always create a visual to accomodate the stats tests
plt.scatter(tc.tenure, tc.total_charges, color='green', alpha=0.3, s=10)
plt.xlabel('Monthly Charges')
plt.ylabel('tenure')
plt.title('Customer retention based on monthly charges')

plt.grid()
plt.show()


# Checked for infinities and NaNs in 'total_charges' and 'tenure' columns
print("Infinities in 'total_charges':", np.isinf(tc_cleaned['total_charges']).any())
print("NaNs in 'total_charges':", np.isnan(tc_cleaned['total_charges']).any())

print("Infinities in 'tenure':", np.isinf(tc_cleaned['tenure']).any())
print("NaNs in 'tenure':", np.isnan(tc_cleaned['tenure']).any())


tc.total_charges = tc.total_charges.fillna(0)


r_tt, p_tt = stats.pearsonr(tc.total_charges, tc.tenure)
r_tt, p_tt


eval_result(p_tt)





url = get_db_url('employees')
query = text('''
        SELECT DISTINCT *, DATEDIFF(s.to_date, s.from_date) AS days
        FROM employees AS e
        LEFT JOIN salaries AS s ON e.emp_no = s.emp_no
        WHERE s.to_date > curdate()
        ''')
engine = create_engine(url)
e_df = pd.read_sql(query, engine.connect())


e_df.info()


e_df.nunique()


# the to_date value reads as '9999-01-01
e_df.head(1)


# datetime lib import
from datetime import datetime

# Converted the date string "2023-08-01" to a datetime.date object
target_date = datetime.strptime("2023-08-01", "%Y-%m-%d").date()

# cur_date variable set to check if the 'to_date' values are greater than the target_date
cur_date = e_df['to_date'] > target_date

# Set the 'to_date' values to current date (2023-08-02 if the condition is True,
# otherwise, keep the original 'to_date' value.
e_df['to_date'] = np.where(cur_date, datetime.strptime("2023-08-02", "%Y-%m-%d").date(), e_df['to_date'])


e_df.head()





# set the emp_dates as number of dates amd them chance to numeric by changing emp_dates using datetime.days an set a new variable
e_df['emp_dates'] = e_df['to_date'] - e_df['from_date']
e_df['emp_days'] = e_df['emp_dates'].dt.days

e_df.head()





#always create a visual to accomodate the stats tests
plt.scatter(e_df.emp_days, e_df.salary , color='grey', alpha=0.3, s=1)
plt.xlabel('Employee Days')
plt.ylabel('Employee Salary')
plt.title('Employee timeframe pay')

plt.grid()
plt.show()





r_es, p_es = stats.pearsonr(e_df.emp_days, e_df.salary)
r_es, p_es


# The result states 'it's significant', but the visual is to crowded to veify a linear relationship 
eval_result(p_es)





url = get_db_url('employees')
query = text('''
        SELECT *, DATEDIFF(t.to_date, t.from_date) AS days
        FROM employees AS e
        JOIN titles AS t ON e.emp_no = t.emp_no
        WHERE t.to_date > curdate()
        ''')
engine = create_engine(url)
e_df = pd.read_sql(query, engine.connect())


e_df.head()


from datetime import datetime

# Converted the date string "2023-08-01" to a datetime.date object
target_date = datetime.strptime("2023-08-01", "%Y-%m-%d").date()

# cur_date variable set to check if the 'to_date' values are greater than the target_date
cur_date = e_df['to_date'] > target_date

# Set the 'to_date' values to current date (2023-08-02 if the condition is True,
# otherwise, keep the original 'to_date' value.
e_df['to_date'] = np.where(cur_date, datetime.strptime("2023-08-02", "%Y-%m-%d").date(), e_df['to_date'])


# set the emp_dates as number of dates amd them chance to numeric by changing emp_dates using datetime.days an set a new variable
e_df['emp_dates'] = e_df['to_date'] - e_df['from_date']
e_df['emp_days'] = e_df['emp_dates'].dt.days

e_df.head()


# Grouped by the index and count the number of unique titles for each employee
emp_titles_count = e_df.groupby(e_df.index)['title'].nunique().reset_index()

# Merged the counts back into the original DataFrame based on the index
e_df = pd.merge(e_df, emp_titles_count, left_index=True, right_on='index', suffixes=('', '_titles'))

# Renamed the newly added column to 'num_titles'
e_df.rename(columns={'title_titles': 'num_titles'}, inplace=True)


e_df.head()


#always create a visual to accomodate the stats tests
plt.scatter(e_df.emp_days, e_df.num_titles , color='cyan', alpha=0.8, s=10)
plt.xlabel('Employee Days')
plt.ylabel('Employee Titles')
plt.title('Employees Titles and Duration')


plt.grid()
plt.show()


r_et, p_et = stats.pearsonr(e_df.emp_days, e_df.title)
r_et, p_et


# The result states 'it's significant', but the visual is to crowded to veify a linear relationship 
eval_result(p_et)





ss = data('sleepstudy')
ss.head()


ss.info()


ss.nunique()


#always create a visual to accomodate the stats tests
plt.scatter(ss.Days, ss.Reaction , color='magenta', alpha=0.8, s=10)
plt.xlabel('Days Awake')
plt.ylabel('Reaction time')
plt.title('Sleepy Study Results')


plt.grid()
plt.show()





r_ss, p_ss = stats.pearsonr(ss.Days, ss.Reaction)
r_ss, p_ss


eval_result(p_ss)








# lib imports
import pandas as pd
import matplotlib.pyplot as plt

from scipy import stats
from pydataset import data






































rng = np.random.default_rng(seed = 42) # create a rand num generator


# create 2 samples or dfs with the information the question provides.


office_one = rng.normal(90, 15, (40))


office_two = rng.normal(100, 20, (50))


ax = plt.axes()
ax.set_facecolor("lightgrey")
plt.hist(office_two, label = 'Office 2',  alpha=0.8)
plt.hist(office_one, label = 'Office 1', alpha=0.5, edgecolor='black')
plt.xlabel('Days')
plt.ylabel('Houses Sold')
plt.title('Average Sales Over Time')
plt.legend()
plt.show()


t_time, p_time = stats.ttest_ind(office_one, office_two)
t_time, p_time


# set alpha

a = 0.05

if p_time < a:

    print('We reject the null hypothesis.')

else:

    print('We fail to reject the null hypothesis.')

p_time








mpg = data('mpg')
mpg.head()


str(mpg.isnull().values.any())








fuel_efficiency = mpg['cty'] + mpg['hwy'] / 2
mpg.head()


cars_2008 = mpg[mpg['year'] == 2008].fuel_efficiency


cars_1999 = mpg[mpg['year'] == 1999].fuel_efficiency


ax = plt.axes()
ax.set_facecolor("lightgrey")
plt.hist(cars_2008, label='2008')
plt.hist(cars_1999, label='1999', alpha=0.5, edgecolor='black')
plt.title('Fuel Efficiency of Vehicles: 2008 vs. 1999')
plt.legend()
plt.show()


# ttest
t_fe, p_fe = stats.ttest_ind(cars_1999, cars_2008)
t_fe, p_fe


# define alpha value ad t condition

a = 0.05

if t_fe > 0 and (p_fe / 2 < a):

    print('We reject the null hypothesis.')

else:

    print('We fail to reject the null hypothesis.')








compact_cars = mpg[mpg['class'] == 'compact'].fuel_efficiency
average_cars = mpg.fuel_efficiency


ax = plt.axes()
ax.set_facecolor("lightgrey")
plt.hist(average_cars, label='Average Cars')
plt.hist(compact_cars, label='Compact Cars', alpha=0.7, edgecolor='black')
plt.title('Fuel Efficiency of Average and Compact Vehicles')
plt.legend()
plt.show()


t_c, p_c = stats.ttest_1samp(compact_cars, average_cars.mean())
t_c, p_c


# set alpha

a = 0.05

if t_c > 0 and (p_c/2 < a):

    print('We reject the null hypothesis.')

else:

    print('We fail to reject the null hypothesis.')

p_time








manual_cars = mpg[mpg['trans'].str.startswith('m')].fuel_efficiency


automatic_cars = mpg[mpg['trans'].str.startswith('a')].fuel_efficiency


ax = plt.axes()
ax.set_facecolor("lightgrey")
plt.hist(automatic_cars, label='Automatic Cars')
plt.hist(manual_cars, label='Manual Cars', alpha=0.7, edgecolor='black')
plt.title('Gas Mileage of Manual & Auto Trans Vehicles')
plt.legend()
plt.show()


t_gm, p_gm = stats.ttest_ind(manual_cars, automatic_cars)
t_gm, p_gm


# define alpha value ad t condition

a = 0.05

if t_gm > 0 and (p_gm / 2 < a):

    print('We reject the null hypothesis.')

else:

    print('We fail to reject the null hypothesis.')
