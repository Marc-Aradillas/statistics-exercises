%autosave 0





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy import stats
from pydataset import data





mpg = data('mpg')
mpg.head()


mpg.info()


mpg.cyl.value_counts()


#always create a visual to accomodate the stats tests

plt.scatter(mpg.cty, mpg.hwy, color='RebeccaPurple')
plt.xlabel('City MPG')
plt.ylabel('Highway MPG')
plt.title('MPG')
plt.grid()
plt.show()


r, p = stats.pearsonr(mpg.cty, mpg.hwy)
r, p





# you can assign default values to a parameters of a function
def eval_result(p_value, a = 0.05):
    
    if p_value < a:

        print('Awesome Sauce! Your result is significant!')
        
    else: 

        print('Your result was not significant!')

# saved alot of development time with this function. 


eval_result(p)








x = list(range(-15, 15, 1))
y = [i ** 3 for i in x]

plt.scatter(x,y)
plt.show()


r_cubed, p_cubed = stats.pearsonr(x,y)
eval_result(p_cubed)


r_cubed #it's telling us there is a linear and strong correlation, but we can obviously tell it is not!





# This is an example of a very deceptive chart

x_not = [1, 2, 3, 4, 5]
y_not = [60, 60.1, 60.2, 60.3, 60.4]

plt.scatter(x_not, y_not)
plt.grid()
plt.show()


# added y limit to verify

x_not = [1, 2, 3, 4, 5]
y_not = [60, 60.1, 60.2, 60.3, 60.4]

plt.scatter(x_not, y_not)
plt.ylim(0, 75)
plt.grid()
plt.show()


r_not, p_not = stats.pearsonr(x_not, y_not)
eval_result(p_not)


r_not # very tightly correlated





# This is a critical concept as we develop in data science

# we want functions for different stats tests


def find_correlation(f_one, f_two):

    r, p = stats.pearsonr(f_one, f_two)

    eval_result(p)


find_correlation(x_not, y_not)
